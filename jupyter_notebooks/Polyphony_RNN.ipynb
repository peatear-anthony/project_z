{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![logo](https://awl.co.jp/wp-content/themes/awl/img/img-header-logo.png)\n",
    "\n",
    "# AI Music Generator Project (Polyphony_RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "import random\n",
    "\n",
    "def hide_toggle(for_next=False):\n",
    "    this_cell = \"\"\"$('div.cell.code_cell.rendered.selected')\"\"\"\n",
    "    next_cell = this_cell + '.next()'\n",
    "\n",
    "    toggle_text = 'Toggle show/hide'  # text shown on toggle link\n",
    "    target_cell = this_cell  # target cell to control with toggle\n",
    "    js_hide_current = ''  # bit of JS to permanently hide code in current cell (only when toggling next cell)\n",
    "\n",
    "    if for_next:\n",
    "        target_cell = next_cell\n",
    "        toggle_text += ' next cell'\n",
    "        js_hide_current = this_cell + '.find(\"div.input\").hide();'\n",
    "\n",
    "    js_f_name = 'code_toggle_{}'.format(str(random.randint(1,2**64)))\n",
    "\n",
    "    html = \"\"\"\n",
    "        <script>\n",
    "            function {f_name}() {{\n",
    "                {cell_selector}.find('div.input').toggle();\n",
    "            }}\n",
    "\n",
    "            {js_hide_current}\n",
    "        </script>\n",
    "\n",
    "        <a href=\"javascript:{f_name}()\">{toggle_text}</a>\n",
    "    \"\"\".format(\n",
    "        f_name=js_f_name,\n",
    "        cell_selector=target_cell,\n",
    "        js_hide_current=js_hide_current, \n",
    "        toggle_text=toggle_text\n",
    "    )\n",
    "\n",
    "    return HTML(html)\n",
    "hide_toggle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurations\n",
    "\n",
    "**Basic**\n",
    "\n",
    "This configuration acts as a baseline for melody generation with an LSTM model. It uses basic one-hot encoding to represent extracted melodies as input to the LSTM. For training, all examples are transposed to the MIDI pitch range [48, 84] and outputs will also be in this range.\n",
    "\n",
    "**Mono**\n",
    "\n",
    "This configuration acts as a baseline for melody generation with an LSTM model. It uses basic one-hot encoding to represent extracted melodies as input to the LSTM. While basic_rnn is trained by transposing all inputs to a narrow range, mono_rnn is able to use the full 128 MIDI pitches.\n",
    "\n",
    "**Lookback**\n",
    "\n",
    "\n",
    "Lookback RNN introduces custom inputs and labels. The custom inputs allow the model to more easily recognize patterns that occur across 1 and 2 bars. They also help the model recognize patterns related to an events position within the measure. The custom labels reduce the amount of information that the RNNâ€™s cell state has to remember by allowing the model to more easily repeat events from 1 and 2 bars ago. This results in melodies that wander less and have a more musical structure.\n",
    "\n",
    "**Attention**\n",
    "\n",
    "In this configuration we introduce the use of attention. Attention allows the model to more easily access past information without having to store that information in the RNN cell's state. This allows the model to more easily learn longer term dependencies, and results in melodies that have longer arching themes.\n",
    "\n",
    "\n",
    "\n",
    "### Training Settings\n",
    "*   **Number of Training Steps:**  Number of update steps to perform, before exiting the loop. This usually should be around 10000 to 20000.\n",
    "*   **Training/Evaluation Ratio:**  How to split your dataset. For example, a ratio of 10% will allocate 90% of your data to training and 10% of your data to evaluation.\n",
    "*   **Batch Size:** Number of examples used in one update step. This value will affect the speed of training. Recommended value is 64\n",
    "\n",
    "## Generate MIDIs from the Trained Model\n",
    "\n",
    "\n",
    "### Generation Settings\n",
    "*  **Output_Folder** : Name of the MIDI Output Folder. It is recommended to change the name for every individual run. For example, \"run1\", \"run2\", and so on...\n",
    "*  **Number_of_MIDIs** : Number of MIDI that will be generated\n",
    "* **Number_of_Steps** :  Length of each generated MIDI. Note that 128 steps is approximately 15 seconds. In addition, if the number of steps is too large, the model may experience difficulty generating the MIDI file.\n",
    "* **Primer_Pitches**: Also known as the **MIDI Note Number**. This is the note/chord which will start the generated sequence. Refer to the images below converting between MIDI Note Numbers and Note Names.\n",
    "\n",
    "![Pitches1](http://c2rexplugins.weebly.com/uploads/1/4/2/6/14264557/627278711.png)\n",
    "![Pitches2](https://raw.githubusercontent.com/Ilya-Simkin/MusicGuru-RNN-Composer/master/images/pianopitchMidi.jpg)\n",
    "\n",
    "[Alternate Reference #1 ](http://www.inspiredacoustics.com/en/MIDI_note_numbers_and_center_frequencies)\n",
    "\n",
    "[Alternate Reference #2 ](http://www.inspiredacoustics.com/en/MIDI_note_numbers_and_center_frequencies) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import widgets\n",
    "\n",
    "\n",
    "training_steps = widgets.IntSlider(\n",
    "    value=500,\n",
    "    min=0,\n",
    "    max=4000,\n",
    "    step=10,\n",
    "    description='Training Steps:',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal',\n",
    "    readout=True,\n",
    "    readout_format='d'\n",
    ")\n",
    "\n",
    "train_validation_percent = widgets.FloatSlider(\n",
    "    value=0.9,\n",
    "    min=0.1,\n",
    "    max=1,\n",
    "    step=0.05,\n",
    "    description='Train/Val Ratio',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal',\n",
    "    readout=True,\n",
    "    readout_format='.2f',\n",
    ")\n",
    "\n",
    "\n",
    "batch_size= widgets.RadioButtons(\n",
    "    options=['64', '128', '264'],\n",
    "    description='Batch Size:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "output_folder = widgets.Text(\n",
    "    value='run1',\n",
    "    placeholder='output folder name',\n",
    "    description='MIDI Generation Folder Name:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "num_midis = widgets.IntSlider(\n",
    "    value=2,\n",
    "    min=1,\n",
    "    max=10,\n",
    "    step=1,\n",
    "    description='Num of Midis:',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal',\n",
    "    readout=True,\n",
    "    readout_format='d'\n",
    ")\n",
    "\n",
    "Number_of_Steps = widgets.IntSlider(\n",
    "    value=128,\n",
    "    min=64,\n",
    "    max=1000,\n",
    "    step=64,\n",
    "    description='Number_of_Steps:',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal',\n",
    "    readout=True,\n",
    "    readout_format='d'\n",
    ")\n",
    "\n",
    "primer_pitches = widgets.Text(\n",
    "        value=\"[60,63]\",\n",
    "        placeholder='Type something',\n",
    "        description='String:',\n",
    "        disabled=False\n",
    "    )\n",
    "\n",
    "primer_pitch = str(primer_pitches.value)\n",
    "\n",
    "display(training_steps)\n",
    "display(train_validation_percent)\n",
    "display(batch_size)\n",
    "display(output_folder)\n",
    "display(num_midis)\n",
    "display(Number_of_Steps)\n",
    "display(primer_pitches)\n",
    "hide_toggle()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "drive_dir = \"/root/magenta-data\"\n",
    "\n",
    "training_dir = os.path.join(drive_dir,\"midi-data/training-set\")\n",
    "generated_midi_dir = os.path.join(drive_dir,\"midi-data/generated_data/polyphony_rnn\")\n",
    "\n",
    "note_dir = os.path.join(drive_dir,\"tmp\")\n",
    "seq_dir = os.path.join(drive_dir,\"tmp/polyphony_rnn/sequence_examples\")\n",
    "\n",
    "os.environ['TRAIN_SET_DIR'] = str(os.path.join(drive_dir,\"midi-data/training-set\"))\n",
    "os.environ['NOTESEQ_FILE'] = str(os.path.join(drive_dir,\"tmp/notesequences.tfrecord\"))\n",
    "os.environ['SEQ_DIR'] = str(os.path.join(drive_dir,\"tmp/polyphony_rnn/sequence_examples\"))\n",
    "os.environ['SEQ_FILE']= str(os.path.join(drive_dir,\"tmp/polyphony_rnn/sequence_examples/training_poly_tracks.tfrecord\"))\n",
    "os.environ['RUN_DIR'] = str(os.path.join(drive_dir,\"tmp/polyphony_rnn/logdir/run1\"))\n",
    "os.environ['OUTPUT_DIR'] = str(os.path.join(drive_dir,\"midi-data/generated_data/polyphony_rnn\"))\n",
    "\n",
    "\n",
    "os.environ['TRAINING_STEPS']=str(training_steps.value)\n",
    "os.environ['TRAIN_VAL_RATIO']= str(train_validation_percent.value)\n",
    "os.environ['BATCH_SIZE']=str(batch_size.value)\n",
    "\n",
    "#hparams\n",
    "hparams='batch_size='+str(batch_size.value) +\",rnn_layer_sizes=\"+str(\"[64,64]\")\n",
    "os.environ['HPARAMS']=hparams\n",
    "\n",
    "# generation\n",
    "generated_midi_dir_temp=os.path.join(generated_midi_dir, output_folder.value)\n",
    "\n",
    "os.environ['OUTPUT_DIR']= generated_midi_dir_temp\n",
    "os.environ['MIDI_OUTPUT']=str(num_midis.value)\n",
    "os.environ['GEN_STEPS']=str(Number_of_Steps.value)\n",
    "os.environ['PRIMER_PITCH']=primer_pitch\n",
    "\n",
    "hide_toggle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!convert_dir_to_note_sequences \\\n",
    "--input_dir=\"$TRAIN_SET_DIR\" \\\n",
    "--output_file=\"$NOTESEQ_FILE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!polyphony_rnn_create_dataset \\\n",
    "--input=\"$NOTESEQ_FILE\" \\\n",
    "--output_dir=\"$SEQ_DIR\" \\\n",
    "--eval_ratio=$TRAIN_VAL_RATIO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!polyphony_rnn_train \\\n",
    "--run_dir=\"$RUN_DIR\" \\\n",
    "--sequence_example_file=\"$SEQ_FILE\" \\\n",
    "--hparams=$HPARAMS \\\n",
    "--num_training_steps=$TRAINING_STEPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!polyphony_rnn_generate \\\n",
    "--run_dir=\"$RUN_DIR\" \\\n",
    "--hparams=$HPARAMS \\\n",
    "--output_dir=\"$OUTPUT_DIR\" \\\n",
    "--num_outputs=$MIDI_OUTPUT \\\n",
    "--num_steps=$GEN_STEPS \\\n",
    "--primer_pitches=$PRIMER_PITCH \\\n",
    "--condition_on_primer=true \\\n",
    "--inject_primer_during_generation=false\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
